<h1 id="trec-2023-product-search-track-guidelines">TREC 2023 Product Search Track Guidelines</h1>
<h2 id="previous-edition">Previous edition</h2>
<h2 id="timetable">Timetable</h2>
<p>Dataset Released</p>
<p>Queries Released</p>
<p>Deadline for submitting ranking runs.</p>
<h2 id="registration">Registration</h2>
<p>To participate in TREC please pre-register at the following website: <a href="https://ir.nist.gov/trecsubmit.open/application.html">https://ir.nist.gov/trecsubmit.open/application.html</a></p>
<h2 id="introduction">Introduction</h2>
<p>The Product Search Track studies information retrieval in a domain of product search. </p>
<h2 id="product-search-track">Product Search Track</h2>
<h3 id="text-only-re-rank">Text Only Re Rank</h3>
<h3 id="text-only-retrieval">Text Only Retrieval</h3>
<h3 id="multi-modal-retrieval">Multi-Modal Retrieval</h3>
<h2 id="datasets">Datasets</h2>
<p>The dataset is </p>
<h3 id="downloading-the-datasets">Downloading the datasets</h3>
<h3 id="document-ranking-dataset">Document ranking dataset</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Filename</th>
<th style="text-align:right">File size</th>
<th style="text-align:right">Num Records</th>
<th>Format</th>
</tr>
</thead>
<tbody>
<tr>
<td>Corpus</td>
<td></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td></td>
</tr>
<tr>
<td>Train</td>
<td></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td></td>
</tr>
<tr>
<td>Dev</td>
<td></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td></td>
</tr>
<tr>
<td>Test (TREC test 2023)</td>
<td></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
</tr>
</tbody>
</table>
<p>The document corpus is in jsonl format. Each document has:</p>
<ul>
<li><p>If you unzip the corpus, you can quickly access a document using:</p>
</li>
</ul>
<h3 id="use-of-external-information">Use of external information</h3>
<p>You are generally allowed to use external information while developing your runs.
When you submit your runs, please fill in a form listing what resources you used.
This could include an external corpus such as Wikipedia or a pretrained model (e.g. word embeddings, BERT).
This could also include the provided set of document ranking training data, but also optionally other data such as the passage ranking task labels or external labels or pretrained models.
This will allow us to analyze the runs and break they down into types.</p>
<h2 id="additional-resources">Additional resources</h2>
<p>We are sharing the following additional resources which we hope will be useful for the community.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Filename</th>
<th style="text-align:right">File size</th>
<th style="text-align:right">Num Records</th>
<th>Format</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
</tr>
</tbody>
</table>
<h2 id="coordinators">Coordinators</h2>
<ul>
<li><a href="https://spacemanidol.com/">Daniel Campos</a> (University of Illinois at Urbana-Champaign)</li>
<li>Corby Rosset (Microsoft)</li>
<li>Alessandro Magnani (Walmart)</li>
<li><a href="http://czhai.cs.illinois.edu/">ChengXiang Zhai</a> (University of Illinois at Urbana-Champaign)</li>
<li>Surya Kallumadi (Lowes)</li>
</ul>
